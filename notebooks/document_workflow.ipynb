{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ“š KM æ–‡ä»¶ç®¡ç†å·¥ä½œæµç¨‹å¯¦ä½œ\n",
                "\n",
                "æœ¬æ•™å­¸å°‡å¸¶ä½ å¯¦ä½œå®Œæ•´çš„æ–‡ä»¶ç®¡ç†å·¥ä½œæµç¨‹ï¼ŒåŒ…å«ï¼š\n",
                "1. è®€å–æ–‡ä»¶æ¸…å–®ï¼ˆManifestï¼‰\n",
                "2. ä¸Šå‚³æ–°æ–‡ä»¶\n",
                "3. AI æ¢æ¬¾åˆ‡åˆ†è™•ç†\n",
                "4. é—œéµå­—èˆ‡å¯¦é«”æŸ¥è©¢ï¼ˆæ¨¡æ“¬ GraphRAGï¼‰\n",
                "5. ç‰ˆæœ¬ç®¡ç†\n",
                "\n",
                "## è³‡æ–™å¤¾çµæ§‹\n",
                "```\n",
                "data/\n",
                "â”œâ”€â”€ manifest.json          # ğŸ“‹ æ–‡ä»¶æ¸…å–®ï¼ˆä¸»ç´¢å¼•ï¼‰\n",
                "â”œâ”€â”€ raw/                   # ğŸ“‚ åŸå§‹æª”æ¡ˆ\n",
                "â””â”€â”€ processed/             # ğŸ¤– AI è™•ç†å¾Œçš„ chunks\n",
                "```\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. ç’°å¢ƒè¨­å®šèˆ‡åŒ¯å…¥\n",
                "\n",
                "é¦–å…ˆï¼Œæˆ‘å€‘éœ€è¦åŒ¯å…¥å¿…è¦çš„ Python å¥—ä»¶ï¼Œä¸¦è¨­å®šè³‡æ–™å¤¾è·¯å¾‘ï¼š\n",
                "\n",
                "- **json**: ç”¨æ–¼è®€å–å’Œå¯«å…¥ JSON æª”æ¡ˆ\n",
                "- **Path**: ç”¨æ–¼è™•ç†æª”æ¡ˆè·¯å¾‘ï¼Œæ¯”å­—ä¸²æ›´å®‰å…¨\n",
                "- **datetime**: ç”¨æ–¼è¨˜éŒ„æ™‚é–“æˆ³è¨˜\n",
                "- **defaultdict**: ç”¨æ–¼å»ºç«‹ç´¢å¼•æ™‚çš„é è¨­å€¼å­—å…¸\n",
                "\n",
                "æˆ‘å€‘æœƒå®šç¾©å››å€‹é‡è¦çš„è·¯å¾‘è®Šæ•¸ï¼š\n",
                "- `DATA_DIR`: è³‡æ–™æ ¹ç›®éŒ„\n",
                "- `RAW_DIR`: åŸå§‹æª”æ¡ˆç›®éŒ„\n",
                "- `PROCESSED_DIR`: AI è™•ç†å¾Œçš„æª”æ¡ˆç›®éŒ„\n",
                "- `MANIFEST_PATH`: æ–‡ä»¶æ¸…å–®æª”æ¡ˆè·¯å¾‘"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import os\n",
                "from datetime import datetime\n",
                "from pathlib import Path\n",
                "from typing import List, Dict, Optional\n",
                "from collections import defaultdict\n",
                "\n",
                "# è¨­å®šè³‡æ–™è·¯å¾‘\n",
                "DATA_DIR = Path(\"../data\")\n",
                "RAW_DIR = DATA_DIR / \"raw\"\n",
                "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
                "MANIFEST_PATH = DATA_DIR / \"manifest.json\"\n",
                "\n",
                "print(f\"ğŸ“ è³‡æ–™ç›®éŒ„: {DATA_DIR.absolute()}\")\n",
                "print(f\"ğŸ“‚ åŸå§‹æª”æ¡ˆ: {RAW_DIR.absolute()}\")\n",
                "print(f\"ğŸ¤– è™•ç†å¾Œæª”æ¡ˆ: {PROCESSED_DIR.absolute()}\")\n",
                "print(f\"ğŸ“‹ æ¸…å–®æª”æ¡ˆ: {MANIFEST_PATH.absolute()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. æ–‡ä»¶æ¸…å–®ç®¡ç†é¡åˆ¥ (ManifestManager)\n",
                "\n",
                "é€™å€‹é¡åˆ¥è² è²¬ç®¡ç† `manifest.json` æ–‡ä»¶æ¸…å–®ï¼Œæä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š\n",
                "\n",
                "| æ–¹æ³• | èªªæ˜ |\n",
                "|------|------|\n",
                "| `list_documents()` | åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶ |\n",
                "| `get_document(doc_id)` | æ ¹æ“š ID å–å¾—æ–‡ä»¶ |\n",
                "| `add_document(doc_info)` | æ–°å¢æ–‡ä»¶ |\n",
                "| `update_document(doc_id, updates)` | æ›´æ–°æ–‡ä»¶è³‡è¨Š |\n",
                "| `search_by_category(category)` | ä¾åˆ†é¡æœå°‹ |\n",
                "| `search_by_keyword(keyword)` | ä¾é—œéµå­—æœå°‹ |\n",
                "\n",
                "### ç‚ºä»€éº¼éœ€è¦é€™å€‹é¡åˆ¥ï¼Ÿ\n",
                "\n",
                "`manifest.json` æ˜¯æ•´å€‹ç³»çµ±çš„ã€Œç›®éŒ„ã€ï¼Œè¨˜éŒ„äº†æ‰€æœ‰æ–‡ä»¶çš„åŸºæœ¬è³‡è¨Šã€ç‰ˆæœ¬æ­·å²ã€è™•ç†ç‹€æ…‹ç­‰ã€‚\n",
                "é€éé€™å€‹é¡åˆ¥ï¼Œæˆ‘å€‘å¯ä»¥æ–¹ä¾¿åœ°æ“ä½œé€™å€‹æ¸…å–®ï¼Œè€Œä¸éœ€è¦æ¯æ¬¡éƒ½æ‰‹å‹•è®€å¯« JSON æª”æ¡ˆã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ManifestManager:\n",
                "    \"\"\"æ–‡ä»¶æ¸…å–®ç®¡ç†å™¨\"\"\"\n",
                "    \n",
                "    def __init__(self, manifest_path: Path):\n",
                "        self.manifest_path = manifest_path\n",
                "        self.manifest = self._load_manifest()\n",
                "    \n",
                "    def _load_manifest(self) -> dict:\n",
                "        \"\"\"è¼‰å…¥æ¸…å–®\"\"\"\n",
                "        if self.manifest_path.exists():\n",
                "            with open(self.manifest_path, 'r', encoding='utf-8') as f:\n",
                "                return json.load(f)\n",
                "        return {\"version\": \"1.0\", \"documents\": []}\n",
                "    \n",
                "    def _save_manifest(self):\n",
                "        \"\"\"å„²å­˜æ¸…å–®\"\"\"\n",
                "        self.manifest[\"updated_at\"] = datetime.now().isoformat()\n",
                "        with open(self.manifest_path, 'w', encoding='utf-8') as f:\n",
                "            json.dump(self.manifest, f, ensure_ascii=False, indent=2)\n",
                "    \n",
                "    def list_documents(self) -> List[dict]:\n",
                "        \"\"\"åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶\"\"\"\n",
                "        return self.manifest.get(\"documents\", [])\n",
                "    \n",
                "    def get_document(self, doc_id: str) -> Optional[dict]:\n",
                "        \"\"\"æ ¹æ“š ID å–å¾—æ–‡ä»¶\"\"\"\n",
                "        for doc in self.manifest[\"documents\"]:\n",
                "            if doc[\"doc_id\"] == doc_id:\n",
                "                return doc\n",
                "        return None\n",
                "    \n",
                "    def add_document(self, doc_info: dict):\n",
                "        \"\"\"æ–°å¢æ–‡ä»¶\"\"\"\n",
                "        self.manifest[\"documents\"].append(doc_info)\n",
                "        self._save_manifest()\n",
                "        print(f\"âœ… å·²æ–°å¢æ–‡ä»¶: {doc_info['doc_id']} - {doc_info['title']}\")\n",
                "    \n",
                "    def update_document(self, doc_id: str, updates: dict):\n",
                "        \"\"\"æ›´æ–°æ–‡ä»¶è³‡è¨Š\"\"\"\n",
                "        for doc in self.manifest[\"documents\"]:\n",
                "            if doc[\"doc_id\"] == doc_id:\n",
                "                doc.update(updates)\n",
                "                self._save_manifest()\n",
                "                print(f\"âœ… å·²æ›´æ–°æ–‡ä»¶: {doc_id}\")\n",
                "                return\n",
                "        print(f\"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {doc_id}\")\n",
                "    \n",
                "    def search_by_category(self, category: str) -> List[dict]:\n",
                "        \"\"\"ä¾åˆ†é¡æœå°‹\"\"\"\n",
                "        return [doc for doc in self.manifest[\"documents\"] \n",
                "                if doc.get(\"category\") == category]\n",
                "    \n",
                "    def search_by_keyword(self, keyword: str) -> List[dict]:\n",
                "        \"\"\"ä¾é—œéµå­—æœå°‹\"\"\"\n",
                "        results = []\n",
                "        for doc in self.manifest[\"documents\"]:\n",
                "            keywords = doc.get(\"metadata\", {}).get(\"keywords\", [])\n",
                "            if keyword in keywords or keyword in doc.get(\"title\", \"\"):\n",
                "                results.append(doc)\n",
                "        return results\n",
                "\n",
                "# å»ºç«‹ç®¡ç†å™¨å¯¦ä¾‹\n",
                "manifest_mgr = ManifestManager(MANIFEST_PATH)\n",
                "print(\"âœ… æ–‡ä»¶æ¸…å–®ç®¡ç†å™¨å·²åˆå§‹åŒ–\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. æª¢è¦–æ–‡ä»¶æ¸…å–®\n",
                "\n",
                "ç¾åœ¨è®“æˆ‘å€‘ä¾†çœ‹çœ‹ç›®å‰ç³»çµ±ä¸­æœ‰å“ªäº›æ–‡ä»¶ã€‚\n",
                "\n",
                "é€™æ®µç¨‹å¼æœƒï¼š\n",
                "1. å‘¼å« `list_documents()` å–å¾—æ‰€æœ‰æ–‡ä»¶\n",
                "2. éæ­·æ¯ä»½æ–‡ä»¶ï¼Œé¡¯ç¤ºåŸºæœ¬è³‡è¨Š\n",
                "3. é¡¯ç¤ºæ¯ä»½æ–‡ä»¶çš„æœ€æ–°ç‰ˆæœ¬è³‡è¨Š\n",
                "\n",
                "ä½ æœƒçœ‹åˆ°é¡ä¼¼é€™æ¨£çš„è¼¸å‡ºï¼š\n",
                "```\n",
                "ğŸ“„ LAW-001: å‹å‹•åŸºæº–æ³•\n",
                "   åˆ†é¡: æ³•è¦ > å‹å‹•æ³•è¦\n",
                "   ç‰ˆæœ¬: v1\n",
                "   ç‹€æ…‹: active\n",
                "   AIè™•ç†: âœ…\n",
                "   Chunks: 15\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶\n",
                "documents = manifest_mgr.list_documents()\n",
                "\n",
                "print(f\"ğŸ“š å…±æœ‰ {len(documents)} ä»½æ–‡ä»¶\\n\")\n",
                "print(\"-\" * 80)\n",
                "\n",
                "for doc in documents:\n",
                "    print(f\"ğŸ“„ {doc['doc_id']}: {doc['title']}\")\n",
                "    print(f\"   åˆ†é¡: {doc['category']} > {doc.get('sub_category', 'N/A')}\")\n",
                "    print(f\"   ç‰ˆæœ¬: v{doc['current_version']}\")\n",
                "    print(f\"   ç‹€æ…‹: {doc['status']}\")\n",
                "    print(f\"   é—œéµå­—: {', '.join(doc.get('metadata', {}).get('keywords', []))}\")\n",
                "    \n",
                "    # é¡¯ç¤ºç‰ˆæœ¬æ­·å²\n",
                "    versions = doc.get('versions', [])\n",
                "    if versions:\n",
                "        latest = versions[-1]\n",
                "        print(f\"   æœ€æ–°ç‰ˆæœ¬:\")\n",
                "        print(f\"      - æª”æ¡ˆ: {latest['file_name']}\")\n",
                "        print(f\"      - AIè™•ç†: {'âœ…' if latest.get('ai_processed') else 'âŒ'}\")\n",
                "        print(f\"      - Chunks: {latest.get('chunk_count', 'N/A')}\")\n",
                "    print(\"-\" * 80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Chunks ç®¡ç†é¡åˆ¥ (ChunksManager)\n",
                "\n",
                "é€™æ˜¯æ•´å€‹ç³»çµ±çš„æ ¸å¿ƒï¼é€™å€‹é¡åˆ¥è² è²¬ç®¡ç† AI è™•ç†å¾Œçš„ chunksï¼Œæ”¯æ´ **GraphRAG é¢¨æ ¼çš„æŸ¥è©¢**ã€‚\n",
                "\n",
                "### ä»€éº¼æ˜¯ Chunkï¼Ÿ\n",
                "\n",
                "Chunk æ˜¯æ–‡ä»¶åˆ‡åˆ†å¾Œçš„æœ€å°å–®ä½ã€‚åœ¨æˆ‘å€‘çš„ç³»çµ±ä¸­ï¼Œæ¯å€‹ã€Œæ¢æ¬¾ã€å°±æ˜¯ä¸€å€‹ chunkã€‚\n",
                "ä¾‹å¦‚å‹åŸºæ³•ç¬¬30æ¢å°±æ˜¯ä¸€å€‹ chunkï¼ŒåŒ…å«ï¼š\n",
                "- æ¢æ¬¾åŸæ–‡\n",
                "- æ‘˜è¦\n",
                "- é—œéµå­—ï¼ˆç”¨æ–¼æœå°‹ï¼‰\n",
                "- å¯¦é«”ï¼ˆè§’è‰²ã€æ•¸å€¼ã€ç¨‹åºç­‰ï¼‰\n",
                "- ç›¸é—œæ¢æ¬¾ï¼ˆç”¨æ–¼ Graph é—œè¯ï¼‰\n",
                "\n",
                "### é€™å€‹é¡åˆ¥å»ºç«‹äº†ä¸‰ç¨®ç´¢å¼•ï¼š\n",
                "\n",
                "| ç´¢å¼•é¡å‹ | èªªæ˜ | ç”¨é€” |\n",
                "|----------|------|------|\n",
                "| `keyword_index` | é—œéµå­— â†’ chunk_ids | é—œéµå­—æœå°‹ |\n",
                "| `entity_index` | å¯¦é«” â†’ chunk_ids | å¯¦é«”æœå°‹ |\n",
                "| `article_index` | æ¢æ¬¾ç·¨è™Ÿ â†’ chunk | ç›´æ¥æŸ¥è©¢ç‰¹å®šæ¢æ¬¾ |\n",
                "\n",
                "é€™äº›ç´¢å¼•è®“æˆ‘å€‘å¯ä»¥å¿«é€Ÿæ‰¾åˆ°ç›¸é—œçš„æ¢æ¬¾ï¼Œä¸éœ€è¦éæ­·æ‰€æœ‰è³‡æ–™ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ChunksManager:\n",
                "    \"\"\"æ¢æ¬¾ Chunks ç®¡ç†å™¨ - æ”¯æ´ GraphRAG æŸ¥è©¢\"\"\"\n",
                "    \n",
                "    def __init__(self, processed_dir: Path):\n",
                "        self.processed_dir = processed_dir\n",
                "        self.chunks_cache = {}\n",
                "        self._build_index()\n",
                "    \n",
                "    def _build_index(self):\n",
                "        \"\"\"å»ºç«‹ç´¢å¼• - é€™æ˜¯ GraphRAG çš„åŸºç¤ï¼\"\"\"\n",
                "        self.keyword_index = defaultdict(list)  # keyword -> [chunk_ids]\n",
                "        self.entity_index = defaultdict(list)   # entity -> [chunk_ids]\n",
                "        self.article_index = {}                 # article_number -> chunk\n",
                "        \n",
                "        # éæ­·æ‰€æœ‰ _chunks.json æª”æ¡ˆ\n",
                "        for json_file in self.processed_dir.glob(\"*_chunks.json\"):\n",
                "            with open(json_file, 'r', encoding='utf-8') as f:\n",
                "                data = json.load(f)\n",
                "                doc_id = data['doc_id']\n",
                "                self.chunks_cache[doc_id] = data\n",
                "                \n",
                "                # å»ºç«‹æ¯å€‹ chunk çš„ç´¢å¼•\n",
                "                for chunk in data.get('chunks', []):\n",
                "                    chunk_id = chunk['chunk_id']\n",
                "                    \n",
                "                    # é—œéµå­—ç´¢å¼•ï¼šè®“æˆ‘å€‘å¯ä»¥ç”¨ã€ŒåŠ ç­ã€æ‰¾åˆ°ç¬¬32æ¢\n",
                "                    for kw in chunk.get('keywords', []):\n",
                "                        self.keyword_index[kw].append(chunk_id)\n",
                "                    \n",
                "                    # å¯¦é«”ç´¢å¼•ï¼šè®“æˆ‘å€‘å¯ä»¥ç”¨ã€Œé›‡ä¸»ã€æ‰¾åˆ°ç›¸é—œæ¢æ¬¾\n",
                "                    for entity in chunk.get('entities', []):\n",
                "                        self.entity_index[entity['value']].append(chunk_id)\n",
                "                    \n",
                "                    # æ¢æ¬¾ç´¢å¼•ï¼šè®“æˆ‘å€‘å¯ä»¥ç›´æ¥æŸ¥è©¢ã€Œç¬¬30æ¢ã€\n",
                "                    self.article_index[chunk['article_number']] = chunk\n",
                "        \n",
                "        print(f\"âœ… ç´¢å¼•å»ºç«‹å®Œæˆ\")\n",
                "        print(f\"   - é—œéµå­—: {len(self.keyword_index)} å€‹\")\n",
                "        print(f\"   - å¯¦é«”: {len(self.entity_index)} å€‹\")\n",
                "        print(f\"   - æ¢æ¬¾: {len(self.article_index)} æ¢\")\n",
                "    \n",
                "    def search_by_keyword(self, keyword: str) -> List[dict]:\n",
                "        \"\"\"ä¾é—œéµå­—æœå°‹ chunks\"\"\"\n",
                "        results = []\n",
                "        for kw, chunk_ids in self.keyword_index.items():\n",
                "            if keyword in kw:  # éƒ¨åˆ†åŒ¹é…\n",
                "                for chunk_id in chunk_ids:\n",
                "                    chunk = self._get_chunk_by_id(chunk_id)\n",
                "                    if chunk and chunk not in results:\n",
                "                        results.append(chunk)\n",
                "        return results\n",
                "    \n",
                "    def search_by_entity(self, entity_value: str) -> List[dict]:\n",
                "        \"\"\"ä¾å¯¦é«”æœå°‹ chunks\"\"\"\n",
                "        results = []\n",
                "        for entity, chunk_ids in self.entity_index.items():\n",
                "            if entity_value in entity:  # éƒ¨åˆ†åŒ¹é…\n",
                "                for chunk_id in chunk_ids:\n",
                "                    chunk = self._get_chunk_by_id(chunk_id)\n",
                "                    if chunk and chunk not in results:\n",
                "                        results.append(chunk)\n",
                "        return results\n",
                "    \n",
                "    def get_article(self, article_number: str) -> Optional[dict]:\n",
                "        \"\"\"å–å¾—ç‰¹å®šæ¢æ¬¾\"\"\"\n",
                "        return self.article_index.get(article_number)\n",
                "    \n",
                "    def get_related_articles(self, article_number: str) -> List[dict]:\n",
                "        \"\"\"å–å¾—ç›¸é—œæ¢æ¬¾ - é€™å°±æ˜¯ Graph æŸ¥è©¢ï¼\"\"\"\n",
                "        chunk = self.get_article(article_number)\n",
                "        if not chunk:\n",
                "            return []\n",
                "        \n",
                "        # æ ¹æ“š related_articles æ¬„ä½æ‰¾åˆ°é—œè¯çš„æ¢æ¬¾\n",
                "        related = []\n",
                "        for rel_article in chunk.get('related_articles', []):\n",
                "            rel_chunk = self.get_article(rel_article)\n",
                "            if rel_chunk:\n",
                "                related.append(rel_chunk)\n",
                "        return related\n",
                "    \n",
                "    def _get_chunk_by_id(self, chunk_id: str) -> Optional[dict]:\n",
                "        \"\"\"æ ¹æ“š chunk_id å–å¾— chunk\"\"\"\n",
                "        doc_id = chunk_id.rsplit('-', 1)[0]\n",
                "        if doc_id in self.chunks_cache:\n",
                "            for chunk in self.chunks_cache[doc_id].get('chunks', []):\n",
                "                if chunk['chunk_id'] == chunk_id:\n",
                "                    return chunk\n",
                "        return None\n",
                "    \n",
                "    def get_all_keywords(self) -> List[str]:\n",
                "        \"\"\"å–å¾—æ‰€æœ‰é—œéµå­—\"\"\"\n",
                "        return list(self.keyword_index.keys())\n",
                "    \n",
                "    def get_all_entities(self) -> List[str]:\n",
                "        \"\"\"å–å¾—æ‰€æœ‰å¯¦é«”\"\"\"\n",
                "        return list(self.entity_index.keys())\n",
                "\n",
                "# å»ºç«‹ Chunks ç®¡ç†å™¨\n",
                "chunks_mgr = ChunksManager(PROCESSED_DIR)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. æŸ¥çœ‹å¯ç”¨çš„é—œéµå­—\n",
                "\n",
                "åœ¨é€²è¡Œæœå°‹ä¹‹å‰ï¼Œè®“æˆ‘å€‘å…ˆçœ‹çœ‹ç³»çµ±ä¸­æœ‰å“ªäº›é—œéµå­—å¯ä¾›æœå°‹ã€‚\n",
                "\n",
                "é€™äº›é—œéµå­—æ˜¯åœ¨ AI è™•ç†éšæ®µå¾æ¢æ¬¾å…§å®¹ä¸­æŠ½å–å‡ºä¾†çš„ï¼Œå¯ä»¥ç”¨ä¾†å¿«é€Ÿå®šä½ç›¸é—œæ¢æ¬¾ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# é¡¯ç¤ºæ‰€æœ‰å¯ç”¨é—œéµå­—\n",
                "print(\"ğŸ·ï¸ å¯ç”¨é—œéµå­—ï¼š\")\n",
                "keywords = chunks_mgr.get_all_keywords()\n",
                "print(\", \".join(keywords[:20]) + \"...\" if len(keywords) > 20 else \", \".join(keywords))\n",
                "print(f\"\\nç¸½å…± {len(keywords)} å€‹é—œéµå­—\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. é—œéµå­—æœå°‹ç¤ºç¯„ï¼šåŠ ç­ç›¸é—œæ¢æ¬¾\n",
                "\n",
                "ç¾åœ¨ä¾†å¯¦éš›æœå°‹çœ‹çœ‹ï¼æˆ‘å€‘ç”¨ã€ŒåŠ ç­ã€é€™å€‹é—œéµå­—ä¾†æ‰¾ç›¸é—œæ¢æ¬¾ã€‚\n",
                "\n",
                "é€™å°±æ˜¯ GraphRAG çš„ç¬¬ä¸€æ­¥ï¼š**å¾é—œéµå­—æ‰¾åˆ°ç›¸é—œçš„ chunks**ã€‚\n",
                "\n",
                "åœ¨å¯¦éš›çš„ RAG ç³»çµ±ä¸­ï¼Œé€™å€‹æ­¥é©Ÿå¯èƒ½æœƒçµåˆå‘é‡æœå°‹ï¼ˆEmbeddingï¼‰ä¾†æé«˜æº–ç¢ºåº¦ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ğŸ” é—œéµå­—æœå°‹ï¼šåŠ ç­ç›¸é—œæ¢æ¬¾\n",
                "print(\"ğŸ” æœå°‹é—œéµå­—ï¼šåŠ ç­\\n\")\n",
                "\n",
                "results = chunks_mgr.search_by_keyword(\"åŠ ç­\")\n",
                "print(f\"æ‰¾åˆ° {len(results)} æ¢ç›¸é—œæ¢æ¬¾ï¼š\\n\")\n",
                "\n",
                "for chunk in results:\n",
                "    print(f\"ğŸ“Œ {chunk['article_number']} - {chunk['title']}\")\n",
                "    print(f\"   ğŸ“ æ‘˜è¦: {chunk['summary']}\")\n",
                "    print(f\"   ğŸ·ï¸ é—œéµå­—: {', '.join(chunk['keywords'])}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. é—œéµå­—æœå°‹ç¤ºç¯„ï¼šç‰¹ä¼‘ç›¸é—œæ¢æ¬¾\n",
                "\n",
                "å†è©¦è©¦æœå°‹ã€Œç‰¹ä¼‘ã€ï¼Œé€™æ˜¯å“¡å·¥æœ€é—œå¿ƒçš„è­°é¡Œä¹‹ä¸€ï¼\n",
                "\n",
                "ä½ æœƒçœ‹åˆ°å‹åŸºæ³•ç¬¬38æ¢çš„å®Œæ•´è³‡è¨Šï¼ŒåŒ…æ‹¬ä¸åŒå¹´è³‡å°æ‡‰çš„ç‰¹ä¼‘å¤©æ•¸ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ğŸ” é—œéµå­—æœå°‹ï¼šç‰¹ä¼‘ç›¸é—œæ¢æ¬¾\n",
                "print(\"ğŸ” æœå°‹é—œéµå­—ï¼šç‰¹ä¼‘\\n\")\n",
                "\n",
                "results = chunks_mgr.search_by_keyword(\"ç‰¹ä¼‘\")\n",
                "for chunk in results:\n",
                "    print(f\"ğŸ“Œ {chunk['article_number']} - {chunk['title']}\")\n",
                "    print(f\"   ğŸ›ï¸ ç« ç¯€: {chunk['chapter']}\")\n",
                "    print(f\"   ğŸ“ æ‘˜è¦: {chunk['summary']}\")\n",
                "    print(f\"\\n   ğŸ“„ æ¢æ–‡å…§å®¹:\")\n",
                "    print(f\"   {chunk['content'][:300]}...\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. æŸ¥è©¢ç‰¹å®šæ¢æ¬¾çš„å®Œæ•´è³‡è¨Š\n",
                "\n",
                "æœ‰æ™‚å€™æˆ‘å€‘éœ€è¦æŸ¥çœ‹ç‰¹å®šæ¢æ¬¾çš„å®Œæ•´è³‡è¨Šï¼ŒåŒ…æ‹¬ï¼š\n",
                "- æ¢æ¬¾åŸæ–‡\n",
                "- æ‘˜è¦\n",
                "- é—œéµå­—\n",
                "- æŠ½å–çš„å¯¦é«”ï¼ˆè§’è‰²ã€æ•¸å€¼ã€ç¨‹åºç­‰ï¼‰\n",
                "- ç›¸é—œæ¢æ¬¾\n",
                "- Metadataï¼ˆæ¢æ¬¾é¡å‹ã€é‡è¦æ€§ã€å¸¸è¦‹å•é¡Œï¼‰\n",
                "\n",
                "é€™äº›è³‡è¨Šåœ¨ GraphRAG ç³»çµ±ä¸­æœƒè¢«ç”¨ä¾†ï¼š\n",
                "1. å»ºç«‹çŸ¥è­˜åœ–è­œçš„ç¯€é»å’Œé‚Š\n",
                "2. ç‚º LLM æä¾›ä¸Šä¸‹æ–‡\n",
                "3. ç”Ÿæˆæ›´æº–ç¢ºçš„å›ç­”"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ğŸ“Š å–å¾—ç‰¹å®šæ¢æ¬¾ä¸¦é¡¯ç¤ºå®Œæ•´è³‡è¨Š\n",
                "print(\"ğŸ“Š æŸ¥è©¢ç¬¬30æ¢ - æ­£å¸¸å·¥ä½œæ™‚é–“\\n\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "article = chunks_mgr.get_article(\"ç¬¬30æ¢\")\n",
                "if article:\n",
                "    print(f\"ğŸ“Œ {article['article_number']} - {article['title']}\")\n",
                "    print(f\"ğŸ›ï¸ ç« ç¯€: {article['chapter']}\")\n",
                "    print(f\"\\nğŸ“ æ¢æ–‡å…§å®¹:\")\n",
                "    print(f\"   {article['content']}\")\n",
                "    print(f\"\\nğŸ“‹ æ‘˜è¦: {article['summary']}\")\n",
                "    print(f\"\\nğŸ·ï¸ é—œéµå­—: {', '.join(article['keywords'])}\")\n",
                "    print(f\"\\nğŸ”— ç›¸é—œæ¢æ¬¾: {', '.join(article['related_articles'])}\")\n",
                "    \n",
                "    # é¡¯ç¤ºå¯¦é«”\n",
                "    print(f\"\\nğŸ¯ æŠ½å–çš„å¯¦é«”:\")\n",
                "    for entity in article['entities']:\n",
                "        print(f\"   [{entity['type']}] {entity['value']}\")\n",
                "    \n",
                "    # é¡¯ç¤º metadata\n",
                "    meta = article.get('metadata', {})\n",
                "    print(f\"\\nğŸ“Š Metadata:\")\n",
                "    print(f\"   é¡å‹: {meta.get('article_type', 'N/A')}\")\n",
                "    print(f\"   é‡è¦æ€§: {meta.get('importance', 'N/A')}\")\n",
                "    if 'common_questions' in meta:\n",
                "        print(f\"   å¸¸è¦‹å•é¡Œ: {', '.join(meta['common_questions'])}\")\n",
                "else:\n",
                "    print(\"âŒ æ‰¾ä¸åˆ°è©²æ¢æ¬¾\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 9. Graph æŸ¥è©¢ï¼šç›¸é—œæ¢æ¬¾ï¼ˆGraphRAG æ ¸å¿ƒåŠŸèƒ½ï¼‰\n",
                "\n",
                "é€™æ˜¯ **GraphRAG æœ€é‡è¦çš„åŠŸèƒ½**ï¼šæ ¹æ“šæ¢æ¬¾ä¹‹é–“çš„é—œè¯é€²è¡ŒæŸ¥è©¢ã€‚\n",
                "\n",
                "### ç‚ºä»€éº¼é€™å¾ˆé‡è¦ï¼Ÿ\n",
                "\n",
                "å‚³çµ±çš„ RAG ç³»çµ±åªèƒ½æ ¹æ“šé—œéµå­—æˆ–å‘é‡ç›¸ä¼¼åº¦æ‰¾åˆ°ç›¸é—œæ–‡ä»¶ã€‚\n",
                "ä½† GraphRAG å¯ä»¥ç†è§£æ–‡ä»¶ä¹‹é–“çš„ã€Œé—œä¿‚ã€ï¼Œä¾‹å¦‚ï¼š\n",
                "- ç¬¬30æ¢ï¼ˆæ­£å¸¸å·¥æ™‚ï¼‰â†’ ç¬¬32æ¢ï¼ˆå»¶é•·å·¥æ™‚/åŠ ç­ï¼‰\n",
                "- ç¬¬30æ¢ï¼ˆæ­£å¸¸å·¥æ™‚ï¼‰â†’ ç¬¬36æ¢ï¼ˆä¾‹å‡ï¼‰\n",
                "\n",
                "é€™æ¨£ç•¶ä½¿ç”¨è€…å•ã€Œæ­£å¸¸å·¥æ™‚æ˜¯å¤šå°‘ï¼ŸåŠ ç­æ€éº¼ç®—ï¼Ÿã€æ™‚ï¼Œ\n",
                "ç³»çµ±å¯ä»¥è‡ªå‹•æ‰¾åˆ°ç›¸é—œçš„æ¢æ¬¾ï¼Œæä¾›æ›´å®Œæ•´çš„ç­”æ¡ˆã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ğŸ”— Graph æŸ¥è©¢ï¼šå–å¾—ç›¸é—œæ¢æ¬¾\n",
                "print(\"ğŸ”— Graph æŸ¥è©¢ï¼šèˆ‡ç¬¬30æ¢ç›¸é—œçš„æ¢æ¬¾\\n\")\n",
                "print(\"é€™å€‹æŸ¥è©¢æ¨¡æ“¬äº† GraphRAG çš„ Graph Traversal åŠŸèƒ½\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "related = chunks_mgr.get_related_articles(\"ç¬¬30æ¢\")\n",
                "print(f\"\\næ‰¾åˆ° {len(related)} æ¢ç›¸é—œæ¢æ¬¾:\\n\")\n",
                "\n",
                "for chunk in related:\n",
                "    print(f\"ğŸ“Œ {chunk['article_number']} - {chunk['title']}\")\n",
                "    print(f\"   ğŸ“ {chunk['summary']}\")\n",
                "    print(f\"   ğŸ·ï¸ é—œéµå­—: {', '.join(chunk['keywords'][:5])}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 10. æŸ¥çœ‹æ‰€æœ‰å¯¦é«”\n",
                "\n",
                "ã€Œå¯¦é«”ã€æ˜¯å¾æ¢æ¬¾ä¸­æŠ½å–çš„é‡è¦æ¦‚å¿µï¼ŒåŒ…æ‹¬ï¼š\n",
                "- **è§’è‰²**ï¼šé›‡ä¸»ã€å‹å·¥\n",
                "- **æ•¸å€¼è¦å®š**ï¼šæ¯æ—¥8å°æ™‚ã€æ¯é€±40å°æ™‚\n",
                "- **ç¨‹åº**ï¼šå·¥æœƒåŒæ„ã€å‹è³‡æœƒè­°åŒæ„\n",
                "- **æ©Ÿæ§‹**ï¼šåŸºæœ¬å·¥è³‡å¯©è­°å§”å“¡æœƒã€è¡Œæ”¿é™¢\n",
                "\n",
                "é€™äº›å¯¦é«”åœ¨ GraphRAG ä¸­æœƒæˆç‚ºçŸ¥è­˜åœ–è­œçš„ç¯€é»ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# é¡¯ç¤ºæ‰€æœ‰å¯¦é«”\n",
                "print(\"ğŸ¯ æ‰€æœ‰æŠ½å–çš„å¯¦é«”ï¼š\\n\")\n",
                "entities = chunks_mgr.get_all_entities()\n",
                "\n",
                "for i, entity in enumerate(entities[:20]):\n",
                "    print(f\"   {i+1:2d}. {entity}\")\n",
                "\n",
                "if len(entities) > 20:\n",
                "    print(f\"\\n   ... å…± {len(entities)} å€‹å¯¦é«”\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 11. å¯¦é«”æœå°‹ç¤ºç¯„\n",
                "\n",
                "ç¾åœ¨ç”¨ã€Œå¯¦é«”ã€ä¾†æœå°‹ç›¸é—œæ¢æ¬¾ã€‚\n",
                "\n",
                "ä¾‹å¦‚æœå°‹ã€Œé›‡ä¸»ã€ï¼Œå¯ä»¥æ‰¾åˆ°æ‰€æœ‰æåˆ°é›‡ä¸»è²¬ä»»ã€ç¾©å‹™çš„æ¢æ¬¾ã€‚\n",
                "é€™å°æ–¼å›ç­”ã€Œé›‡ä¸»éœ€è¦è² æ“”å“ªäº›è²¬ä»»ï¼Ÿã€é€™é¡å•é¡Œéå¸¸æœ‰ç”¨ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ğŸ” å¯¦é«”æœå°‹ï¼šé›‡ä¸»ç›¸é—œæ¢æ¬¾\n",
                "print(\"ğŸ” æœå°‹å¯¦é«”ï¼šé›‡ä¸»\\n\")\n",
                "\n",
                "results = chunks_mgr.search_by_entity(\"é›‡ä¸»\")\n",
                "print(f\"æ‰¾åˆ° {len(results)} æ¢ç›¸é—œæ¢æ¬¾ï¼ˆé¡¯ç¤ºå‰5æ¢ï¼‰:\\n\")\n",
                "\n",
                "for chunk in results[:5]:\n",
                "    print(f\"ğŸ“Œ {chunk['article_number']} - {chunk['title']}\")\n",
                "    print(f\"   ğŸ“ {chunk['summary']}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 12. æ¨¡æ“¬ AI è™•ç†æµç¨‹\n",
                "\n",
                "é€™å€‹å‡½å¼æ¨¡æ“¬å®Œæ•´çš„ AI è™•ç†æµç¨‹ï¼Œå±•ç¤ºå¦‚ä½•å°‡åŸå§‹æ–‡ä»¶è½‰æ›æˆ chunksã€‚\n",
                "\n",
                "### å¯¦éš›æ‡‰ç”¨ä¸­çš„ AI è™•ç†æ­¥é©Ÿï¼š\n",
                "\n",
                "1. **æ¢æ¬¾åˆ‡åˆ†**ï¼šä½¿ç”¨ LLM æˆ–è¦å‰‡å°‡æ–‡ä»¶åˆ‡æˆæ¢æ¬¾\n",
                "2. **æ‘˜è¦ç”Ÿæˆ**ï¼šä½¿ç”¨ LLM ç‚ºæ¯å€‹æ¢æ¬¾ç”Ÿæˆæ‘˜è¦\n",
                "3. **é—œéµå­—æŠ½å–**ï¼šä½¿ç”¨ NER æˆ– LLM æŠ½å–é—œéµå­—\n",
                "4. **å¯¦é«”è­˜åˆ¥**ï¼šè­˜åˆ¥è§’è‰²ã€æ•¸å€¼ã€ç¨‹åºç­‰å¯¦é«”\n",
                "5. **é—œè¯å»ºç«‹**ï¼šåˆ†ææ¢æ¬¾ä¹‹é–“çš„å¼•ç”¨é—œä¿‚\n",
                "\n",
                "é€™å€‹æ¨¡æ“¬ç‰ˆæœ¬æœƒç›´æ¥ä½¿ç”¨åŸå§‹è³‡æ–™ä¸­çš„ keywordsï¼Œ\n",
                "å¯¦éš›æ‡‰ç”¨æ™‚æœƒå‘¼å« OpenAIã€Claude ç­‰ LLM APIã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def simulate_ai_processing(raw_file_path: Path, output_path: Path) -> dict:\n",
                "    \"\"\"\n",
                "    æ¨¡æ“¬ AI è™•ç†æµç¨‹\n",
                "    \n",
                "    å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œé€™è£¡æœƒå‘¼å« LLM API é€²è¡Œï¼š\n",
                "    1. æ¢æ¬¾åˆ‡åˆ†\n",
                "    2. æ‘˜è¦ç”Ÿæˆ\n",
                "    3. é—œéµå­—æŠ½å–\n",
                "    4. å¯¦é«”è­˜åˆ¥\n",
                "    5. ç›¸é—œæ¢æ¬¾é—œè¯\n",
                "    \"\"\"\n",
                "    print(f\"ğŸ¤– é–‹å§‹ AI è™•ç†: {raw_file_path.name}\")\n",
                "    print(\"   1ï¸âƒ£ è®€å–åŸå§‹æª”æ¡ˆ...\")\n",
                "    \n",
                "    with open(raw_file_path, 'r', encoding='utf-8') as f:\n",
                "        data = json.load(f)\n",
                "    \n",
                "    print(f\"   2ï¸âƒ£ è§£ææ¢æ¬¾... (å…± {len(data.get('articles', []))} æ¢)\")\n",
                "    print(\"   3ï¸âƒ£ ç”Ÿæˆæ‘˜è¦...ï¼ˆæ¨¡æ“¬ï¼‰\")\n",
                "    print(\"   4ï¸âƒ£ æŠ½å–é—œéµå­—...\")\n",
                "    print(\"   5ï¸âƒ£ è­˜åˆ¥å¯¦é«”...ï¼ˆæ¨¡æ“¬ï¼‰\")\n",
                "    print(\"   6ï¸âƒ£ å»ºç«‹æ¢æ¬¾é—œè¯...ï¼ˆæ¨¡æ“¬ï¼‰\")\n",
                "    \n",
                "    # æ¨¡æ“¬è™•ç†çµæœ\n",
                "    result = {\n",
                "        \"doc_id\": \"NEW-001\",\n",
                "        \"title\": data.get(\"law_name\", \"æœªçŸ¥æ–‡ä»¶\"),\n",
                "        \"version\": 1,\n",
                "        \"processed_at\": datetime.now().isoformat(),\n",
                "        \"total_chunks\": len(data.get('articles', [])),\n",
                "        \"chunks\": [],\n",
                "        \"ai_metadata\": {\n",
                "            \"model\": \"gpt-4 (æ¨¡æ“¬)\",\n",
                "            \"processing_time_ms\": 5000\n",
                "        }\n",
                "    }\n",
                "    \n",
                "    # è½‰æ›æ¯å€‹æ¢æ¬¾ç‚º chunk\n",
                "    for article in data.get('articles', []):\n",
                "        chunk = {\n",
                "            \"chunk_id\": f\"NEW-001-{article['article_id'].replace('ç¬¬', '').replace('æ¢', '').zfill(3)}\",\n",
                "            \"article_number\": article['article_id'],\n",
                "            \"chapter\": article.get('chapter', ''),\n",
                "            \"title\": article.get('title', ''),\n",
                "            \"content\": article.get('content', ''),\n",
                "            \"summary\": f\"ï¼ˆAI ç”Ÿæˆæ‘˜è¦ï¼‰{article.get('content', '')[:50]}...\",\n",
                "            \"keywords\": article.get('keywords', []),\n",
                "            \"entities\": [],  # å¯¦éš›æ‡‰ç”¨ä¸­æœƒç”± AI ç”Ÿæˆ\n",
                "            \"related_articles\": [],  # å¯¦éš›æ‡‰ç”¨ä¸­æœƒç”± AI åˆ†æ\n",
                "            \"metadata\": {\n",
                "                \"importance\": article.get('importance', 'medium')\n",
                "            }\n",
                "        }\n",
                "        result[\"chunks\"].append(chunk)\n",
                "    \n",
                "    # å„²å­˜è™•ç†çµæœ\n",
                "    with open(output_path, 'w', encoding='utf-8') as f:\n",
                "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
                "    \n",
                "    print(f\"\\nâœ… AI è™•ç†å®Œæˆï¼\")\n",
                "    print(f\"   ğŸ“ è¼¸å‡ºæª”æ¡ˆ: {output_path}\")\n",
                "    print(f\"   ğŸ“Š ç¸½ chunks: {result['total_chunks']}\")\n",
                "    \n",
                "    return result\n",
                "\n",
                "print(\"âœ… AI è™•ç†å‡½å¼å·²å®šç¾©\")\n",
                "print(\"\\nğŸ’¡ ä½¿ç”¨æ–¹å¼:\")\n",
                "print('   result = simulate_ai_processing(')\n",
                "print('       RAW_DIR / \"å‹å‹•åŸºæº–æ³•.json\",')\n",
                "print('       PROCESSED_DIR / \"test_chunks.json\"')\n",
                "print('   )')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 13. æ–°å¢æ–‡ä»¶æµç¨‹ç¤ºç¯„\n",
                "\n",
                "é€™å€‹å‡½å¼å±•ç¤ºå®Œæ•´çš„æ–°å¢æ–‡ä»¶æµç¨‹ï¼š\n",
                "\n",
                "1. **å»ºç«‹æ–‡ä»¶è³‡è¨Š** - è¨­å®š doc_idã€æ¨™é¡Œã€åˆ†é¡ç­‰\n",
                "2. **æª¢æŸ¥åŸå§‹æª”æ¡ˆ** - ç¢ºèªæª”æ¡ˆå·²æ”¾å…¥ raw/ è³‡æ–™å¤¾\n",
                "3. **AI è™•ç†** - å‘¼å« AI é€²è¡Œæ¢æ¬¾åˆ‡åˆ†\n",
                "4. **æ›´æ–°æ¸…å–®** - å°‡æ–‡ä»¶è³‡è¨ŠåŠ å…¥ manifest.json\n",
                "\n",
                "é€™å€‹å‡½å¼ç›®å‰åªæ˜¯å±•ç¤ºæµç¨‹ï¼Œå¯¦éš›åŸ·è¡Œéœ€è¦å–æ¶ˆè¨»è§£ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def add_new_document(\n",
                "    doc_id: str,\n",
                "    title: str,\n",
                "    category: str,\n",
                "    sub_category: str,\n",
                "    department: str,\n",
                "    raw_file_name: str,\n",
                "    keywords: List[str],\n",
                "    uploaded_by: str = \"admin\"\n",
                ") -> dict:\n",
                "    \"\"\"\n",
                "    å®Œæ•´çš„æ–°å¢æ–‡ä»¶æµç¨‹\n",
                "    \"\"\"\n",
                "    print(f\"ğŸ“„ æ–°å¢æ–‡ä»¶: {doc_id} - {title}\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    # 1. å»ºç«‹æ–‡ä»¶è³‡è¨Š\n",
                "    doc_info = {\n",
                "        \"doc_id\": doc_id,\n",
                "        \"title\": title,\n",
                "        \"category\": category,\n",
                "        \"sub_category\": sub_category,\n",
                "        \"department\": department,\n",
                "        \"current_version\": 1,\n",
                "        \"status\": \"active\",\n",
                "        \"versions\": [\n",
                "            {\n",
                "                \"version\": 1,\n",
                "                \"file_name\": raw_file_name,\n",
                "                \"raw_path\": f\"raw/{raw_file_name}\",\n",
                "                \"processed_path\": f\"processed/{doc_id}_chunks.json\",\n",
                "                \"uploaded_at\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
                "                \"uploaded_by\": uploaded_by,\n",
                "                \"description\": \"åˆç‰ˆ\",\n",
                "                \"ai_processed\": False,\n",
                "                \"chunk_count\": 0\n",
                "            }\n",
                "        ],\n",
                "        \"metadata\": {\n",
                "            \"keywords\": keywords\n",
                "        }\n",
                "    }\n",
                "    \n",
                "    print(f\"\\n1ï¸âƒ£ å»ºç«‹æ–‡ä»¶è³‡è¨Š\")\n",
                "    print(f\"   æ–‡ä»¶ç·¨è™Ÿ: {doc_id}\")\n",
                "    print(f\"   åˆ†é¡: {category} > {sub_category}\")\n",
                "    \n",
                "    # 2. æª¢æŸ¥åŸå§‹æª”æ¡ˆæ˜¯å¦å­˜åœ¨\n",
                "    raw_path = RAW_DIR / raw_file_name\n",
                "    print(f\"\\n2ï¸âƒ£ æª¢æŸ¥åŸå§‹æª”æ¡ˆ\")\n",
                "    if raw_path.exists():\n",
                "        print(f\"   âœ… æª”æ¡ˆå­˜åœ¨: {raw_path}\")\n",
                "    else:\n",
                "        print(f\"   âš ï¸ æª”æ¡ˆä¸å­˜åœ¨: {raw_path}\")\n",
                "        print(f\"   è«‹å…ˆå°‡æª”æ¡ˆæ”¾å…¥ raw/ è³‡æ–™å¤¾\")\n",
                "        return doc_info\n",
                "    \n",
                "    # 3. AI è™•ç†ï¼ˆé€™è£¡åªé¡¯ç¤ºèªªæ˜ï¼Œå¯¦éš›åŸ·è¡Œè¦å–æ¶ˆè¨»è§£ï¼‰\n",
                "    print(f\"\\n3ï¸âƒ£ AI è™•ç†\")\n",
                "    print(f\"   â³ å¾…è™•ç†\")\n",
                "    # processed_path = PROCESSED_DIR / f\"{doc_id}_chunks.json\"\n",
                "    # result = simulate_ai_processing(raw_path, processed_path)\n",
                "    # doc_info['versions'][0]['ai_processed'] = True\n",
                "    # doc_info['versions'][0]['chunk_count'] = result['total_chunks']\n",
                "    \n",
                "    # 4. æ›´æ–°æ¸…å–®ï¼ˆé€™è£¡åªé¡¯ç¤ºèªªæ˜ï¼Œå¯¦éš›åŸ·è¡Œè¦å–æ¶ˆè¨»è§£ï¼‰\n",
                "    print(f\"\\n4ï¸âƒ£ æ›´æ–°æ¸…å–®\")\n",
                "    print(f\"   â³ å¾…æ›´æ–°\")\n",
                "    # manifest_mgr.add_document(doc_info)\n",
                "    \n",
                "    print(f\"\\nâœ… æµç¨‹å±•ç¤ºå®Œæˆï¼\")\n",
                "    print(f\"\\nğŸ’¡ å¯¦éš›åŸ·è¡Œæ™‚ï¼Œè«‹å–æ¶ˆä¸Šé¢ç¨‹å¼ç¢¼ä¸­çš„è¨»è§£\")\n",
                "    return doc_info\n",
                "\n",
                "print(\"âœ… æ–°å¢æ–‡ä»¶å‡½å¼å·²å®šç¾©\")\n",
                "print(\"\\nğŸ’¡ ä½¿ç”¨ç¯„ä¾‹:\")\n",
                "print('''   add_new_document(\n",
                "       doc_id=\"HR-002\",\n",
                "       title=\"å“¡å·¥è€ƒå‹¤ç®¡ç†è¾¦æ³•\",\n",
                "       category=\"äººåŠ›è³‡æº\",\n",
                "       sub_category=\"è€ƒå‹¤è¦å®š\",\n",
                "       department=\"äººåŠ›è³‡æºéƒ¨\",\n",
                "       raw_file_name=\"è€ƒå‹¤ç®¡ç†è¾¦æ³•.json\",\n",
                "       keywords=[\"è€ƒå‹¤\", \"æ‰“å¡\", \"é²åˆ°\", \"å‡ºå‹¤\"]\n",
                "   )''')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 14. çµ±è¨ˆè³‡è¨Š\n",
                "\n",
                "æœ€å¾Œï¼Œè®“æˆ‘å€‘çœ‹çœ‹æ•´å€‹ç³»çµ±çš„çµ±è¨ˆè³‡è¨Šï¼š\n",
                "- ç¸½æ–‡ä»¶æ•¸\n",
                "- åˆ†é¡çµ±è¨ˆ\n",
                "- Chunks çµ±è¨ˆ\n",
                "- é—œéµå­—å’Œå¯¦é«”æ•¸é‡\n",
                "\n",
                "é€™äº›è³‡è¨Šå¯ä»¥å¹«åŠ©ä½ äº†è§£çŸ¥è­˜åº«çš„è¦æ¨¡å’Œè¦†è“‹ç¯„åœã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# çµ±è¨ˆè³‡è¨Š\n",
                "print(\"ğŸ“Š ç³»çµ±çµ±è¨ˆ\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# æ–‡ä»¶çµ±è¨ˆ\n",
                "documents = manifest_mgr.list_documents()\n",
                "print(f\"\\nğŸ“š æ–‡ä»¶çµ±è¨ˆ:\")\n",
                "print(f\"   ç¸½æ–‡ä»¶æ•¸: {len(documents)}\")\n",
                "\n",
                "# åˆ†é¡çµ±è¨ˆ\n",
                "categories = defaultdict(int)\n",
                "for doc in documents:\n",
                "    categories[doc.get('category', 'æœªåˆ†é¡')] += 1\n",
                "\n",
                "print(f\"\\nğŸ·ï¸ åˆ†é¡çµ±è¨ˆ:\")\n",
                "for cat, count in categories.items():\n",
                "    print(f\"   {cat}: {count} ä»½\")\n",
                "\n",
                "# Chunks çµ±è¨ˆ\n",
                "total_chunks = sum(\n",
                "    doc['versions'][-1].get('chunk_count', 0) \n",
                "    for doc in documents if doc.get('versions')\n",
                ")\n",
                "\n",
                "print(f\"\\nğŸ§© Chunks çµ±è¨ˆ:\")\n",
                "print(f\"   ç¸½ chunks: {total_chunks}\")\n",
                "print(f\"   é—œéµå­—æ•¸: {len(chunks_mgr.get_all_keywords())}\")\n",
                "print(f\"   å¯¦é«”æ•¸: {len(chunks_mgr.get_all_entities())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ“ ç¸½çµ\n",
                "\n",
                "æœ¬æ•™å­¸æ¶µè“‹äº†å®Œæ•´çš„æ–‡ä»¶ç®¡ç†å·¥ä½œæµç¨‹ï¼š\n",
                "\n",
                "### å­¸åˆ°çš„å…§å®¹\n",
                "\n",
                "| ç« ç¯€ | å…§å®¹ |\n",
                "|------|------|\n",
                "| 1-3 | Manifest æ–‡ä»¶æ¸…å–®ç®¡ç† |\n",
                "| 4-5 | Chunks ç®¡ç†èˆ‡ç´¢å¼•å»ºç«‹ |\n",
                "| 6-7 | é—œéµå­—æœå°‹ |\n",
                "| 8-9 | æ¢æ¬¾æŸ¥è©¢èˆ‡ Graph é—œè¯ |\n",
                "| 10-11 | å¯¦é«”æœå°‹ |\n",
                "| 12-13 | AI è™•ç†èˆ‡æ–°å¢æ–‡ä»¶æµç¨‹ |\n",
                "| 14 | çµ±è¨ˆè³‡è¨Š |\n",
                "\n",
                "### ä¸‹ä¸€æ­¥\n",
                "\n",
                "- ğŸ”Œ æ•´åˆå¯¦éš›çš„ LLM APIï¼ˆOpenAIã€Claudeï¼‰é€²è¡Œæ¢æ¬¾åˆ‡åˆ†\n",
                "- ğŸ“Š åŠ å…¥å‘é‡åµŒå…¥ï¼ˆEmbeddingï¼‰æ”¯æ´èªæ„æœå°‹\n",
                "- ğŸ•¸ï¸ å»ºç«‹å®Œæ•´çš„çŸ¥è­˜åœ–è­œï¼ˆKnowledge Graphï¼‰\n",
                "- ğŸ’¾ æ•´åˆ MongoDB å„²å­˜\n",
                "- ğŸ¤– çµåˆ LangChain æˆ– LlamaIndex å»ºç«‹å®Œæ•´çš„ RAG ç³»çµ±"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.12 (ai-asst-db)",
            "language": "python",
            "name": "ai-asst-db"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}